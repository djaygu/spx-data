# Story 1.4: Data Pipeline with Streaming

**Story ID**: 1.4  
**Epic**: Epic 1 - Foundation & Streaming Pipeline  
**Status**: Draft  
**Priority**: High  
**Estimated Effort**: 3-4 days  
**Created**: 2025-08-07  
**Approved**: Pending  

## User Story

**As a** system operator,  
**I want** to stream and process Greeks data as each expiration completes,  
**So that** I can handle large datasets without memory constraints while writing to persistent storage formats.

## Context

Following the completion of Stories 1.1, 1.2, and 1.3, we now have:
- Effect-TS project structure with service layers and test framework
- ThetaDataApiClient that returns complete CSV data per expiration (not streamed)
- BulkGreeksProcessor that processes multiple expirations in parallel but returns all data at once
- Metrics collection and progress tracking capabilities
- Proven concurrency patterns with Effect.all

**The Problem**: The current `processBulkGreeks` method waits for ALL expirations to complete before returning, holding all data in memory. With 30+ expirations, this can be 150,000+ records consuming significant memory.

**The Solution**: Enhance BulkGreeksProcessor with streaming methods that yield results as each expiration completes, then create a DataPipeline service to handle writing to CSV/Parquet formats.

## Acceptance Criteria

[ ] 1. Add `streamBulkGreeks` method to BulkGreeksProcessor that returns Stream<ExpirationResult>
[ ] 2. Create DataWriter abstraction with CsvDataWriter implementation
[ ] 3. Create DataPipeline service that orchestrates streaming with DataWriter
[ ] 4. Process and write data in configurable chunks (default: 1000 records)
[ ] 5. Write CSV files organized by expiration date with proper headers
[ ] 6. Free memory after each expiration is written (no accumulation)
[ ] 7. Provide real-time progress updates with throughput metrics
[ ] 8. Support graceful interruption with proper resource cleanup
[ ] 9. Ensure atomic file writes to prevent corruption

## Technical Implementation Tasks

### TDD Workflow Order:
1. Enhance BulkGreeksProcessor interface with streaming methods
2. Create DataPipeline service interface
3. Write tests for new streaming functionality
4. Implement Live layers to make tests pass

### Task 1: Enhance BulkGreeksProcessor with Streaming (AC: 1)
- [ ] Add `streamBulkGreeks` method to BulkGreeksProcessor interface
- [ ] Abstract common processing logic between batch and stream methods
- [ ] Update BulkGreeksProcessorLive to implement streaming method
- [ ] Reuse `filterExpirationsInternal` and error handling logic
- [ ] Update BulkGreeksProcessorTest to mock streaming behavior

### Task 2: DataWriter Service Interface (AC: 2)
- [ ] Define DataWriter service interface in `src/services/DataWriter.ts`
- [ ] Create Context.Tag following Effect-TS patterns
- [ ] Define WriteMetadata type for chunk information
- [ ] Define WriteResult type for completion data
- [ ] Create DataWriterError tagged error type

### Task 3: DataPipeline Service Interface (AC: 2, 6)
- [ ] Define DataPipeline service interface in `src/services/DataPipeline.ts`
- [ ] Create simplified process method that accepts stream and config
- [ ] Define PipelineConfig type
- [ ] Define PipelineProgress type for metrics
- [ ] Create DataPipelineError tagged error type

### Task 4: CSV DataWriter Implementation (AC: 4, 8)
- [ ] Create CsvDataWriter in `src/layers/CsvDataWriter.ts`
- [ ] Implement streaming CSV write with headers
- [ ] Add file organization by expiration date (YYYY-MM-DD format)
- [ ] Implement atomic writes with temporary files
- [ ] Handle first chunk (headers) vs subsequent chunks

### Task 5: Test Layer Implementation (TDD Step 2)
- [ ] Update BulkGreeksProcessorTest with streaming mocks
- [ ] Create DataWriterTest in `src/layers/DataWriterTest.ts`
- [ ] Mock write operations with memory storage
- [ ] Track chunks written for verification
- [ ] Simulate write failures for error testing

### Task 6: Test Suite Creation (TDD Step 3)
- [ ] Write streaming tests in `test/services/BulkGreeksProcessor.test.ts`
- [ ] Write tests in `test/services/DataWriter.test.ts`
- [ ] Write tests in `test/services/DataPipeline.test.ts`
- [ ] Test streaming vs batch method equivalence
- [ ] Test DataWriter contract with mock implementation
- [ ] Test DataPipeline orchestration logic
- [ ] Test memory usage stays bounded

### Task 7: DataPipeline Implementation (AC: 3, 5)
- [ ] Create DataPipelineLive in `src/layers/DataPipelineLive.ts`
- [ ] Implement chunking with configurable size
- [ ] Inject DataWriter dependency
- [ ] Add progress tracking with Ref
- [ ] Implement throughput calculation

### Task 8: Progress and Metrics (AC: 6)
- [ ] Implement real-time progress tracking in DataPipeline
- [ ] Calculate throughput (records/second)
- [ ] Track processing time per expiration
- [ ] Update progress on each chunk processed

### Task 9: Resource Management (AC: 7)
- [ ] Implement graceful shutdown in CsvDataWriter
- [ ] Ensure file handles are properly closed
- [ ] Clean up temporary files on error
- [ ] Add resource finalizers using Effect.acquireRelease

### Task 10: Integration (TDD Step 4)
- [ ] Enhance BulkGreeksProcessorLive with streaming method
- [ ] Wire up DataPipelineLive with CsvDataWriter
- [ ] Test full pipeline with mock data
- [ ] Ensure all unit tests pass

### Task 11: Integration Tests
- [ ] Update `test/integration/BulkGreeksProcessorLive.integration.test.ts` with streaming tests
- [ ] Create `test/integration/DataPipelineLive.integration.test.ts`
- [ ] Test with real ThetaData Terminal data
- [ ] Verify CSV file output correctness
- [ ] Validate memory usage stays bounded
- [ ] Test with multiple expirations

### Task 12: Documentation and Examples
- [ ] Document DataWriter interface for future Parquet implementation
- [ ] Document DataPipeline usage patterns
- [ ] Create example showing CSV writer usage
- [ ] Prepare for CLI integration in next story

## Dev Notes

### Architecture Context from Previous Stories

**Service Layer Pattern (from Stories 1.1-1.3):**
- Use Effect Context.Tag for service definition  
- Create Live and Test layer implementations
- Follow dependency injection pattern
- Maintain separation of concerns

**Architecture Decision - Option A:**
We're enhancing BulkGreeksProcessor with streaming methods rather than creating a separate service. This maximizes code reuse:
- Reuse `filterExpirationsInternal` logic
- Reuse error handling and retry logic
- Reuse progress tracking infrastructure
- Abstract common processing logic for both batch and stream methods

**Key Implementation Details:**
```typescript
// Enhanced BulkGreeksProcessor interface
interface BulkGreeksProcessor {
  // Existing batch method (backward compatible)
  processBulkGreeks(params): Effect<ProcessingMetrics, ...>
  
  // NEW: Stream results as they complete
  streamBulkGreeks(params): Stream<ExpirationResult, ...>
}

// DataWriter service - abstraction for different formats
class DataWriter extends Context.Tag('DataWriter')<
  DataWriter,
  {
    writeChunk(
      chunk: ReadonlyArray<OptionsGreeksData>,
      metadata: WriteMetadata
    ): Effect<void, DataWriterError>
    
    finalize(): Effect<WriteResult, DataWriterError>
    
    getFormat(): string  // 'csv' or 'parquet'
  }
>() {}

// DataPipeline service - orchestrates streaming and writing
class DataPipeline extends Context.Tag('DataPipeline')<
  DataPipeline,
  {
    process(
      dataStream: Stream<OptionsGreeksData, never>,
      config: PipelineConfig
    ): Effect<void, DataPipelineError>
    
    getProgress(): Effect<PipelineProgress | undefined, never>
  }
>() {}

// Writer implementations
const CsvDataWriter = Layer.effect(DataWriter, ...)  // CSV implementation
const ParquetDataWriter = Layer.effect(DataWriter, ...) // Future: Parquet implementation

// Configuration types
interface PipelineConfig {
  outputDir: string           // Default: ./data/greeks
  chunkSize: number          // Default: 1000
  compression: boolean       // Default: false for CSV
  fileNamePattern: string    // e.g., "SPXW_{expiration}.csv"
}

interface WriteMetadata {
  expiration: string
  isFirstChunk: boolean
  isLastChunk: boolean
  chunkIndex: number
}
```

**Implementation Pattern:**
```typescript
// Abstracted common logic in BulkGreeksProcessorLive
const processExpiration = (exp: ExpirationDate): Effect<ExpirationResult> => {
  // Common logic for both batch and stream
  return apiClient.getBulkOptionsGreeks(params).pipe(
    Effect.map(data => ({ expiration, data, success: true })),
    Effect.catchAll(error => Effect.succeed({ expiration, error, success: false }))
  )
}

// Batch method (existing)
processBulkGreeks: (params) => Effect.gen(function* (_) {
  const results = yield* _(
    Effect.all(expirations.map(processExpiration), { concurrency })
  )
  return calculateMetrics(results)
})

// Stream method (new)
streamBulkGreeks: (params) => Stream.fromIterable(expirations).pipe(
  Stream.mapEffect(processExpiration, { concurrency })
)

// DataPipeline implementation with DataWriter
const DataPipelineLive = Layer.effect(
  DataPipeline,
  Effect.gen(function* (_) {
    const writer = yield* _(DataWriter)  // Injected dependency
    const progressRef = yield* _(Ref.make<PipelineProgress>(...))
    
    return DataPipeline.of({
      process: (dataStream, config) =>
        dataStream.pipe(
          Stream.chunks(config.chunkSize),
          Stream.zipWithIndex,
          Stream.tap(([chunk, index]) =>
            writer.writeChunk(chunk, {
              expiration: getCurrentExpiration(),
              isFirstChunk: index === 0,
              isLastChunk: false, // Set on stream end
              chunkIndex: index
            })
          ),
          Stream.tap(() => updateProgress(progressRef)),
          Stream.runDrain
        )
    })
  })
)

// Usage with different writers
const csvPipeline = DataPipelineLive.pipe(
  Layer.provide(CsvDataWriter)
)

const parquetPipeline = DataPipelineLive.pipe(
  Layer.provide(ParquetDataWriter)  // Future
)
```

**Project Structure:**
```
src/
  services/
    BulkGreeksProcessor.ts    # ENHANCED with streaming
    DataPipeline.ts           # NEW orchestration service
    DataWriter.ts             # NEW abstraction for formats
  layers/
    BulkGreeksProcessorLive.ts # ENHANCED implementation
    DataPipelineLive.ts       # NEW orchestration layer
    CsvDataWriter.ts          # CSV implementation of DataWriter
    ParquetDataWriter.ts      # FUTURE: Parquet implementation
test/
  services/
    BulkGreeksProcessor.test.ts # ENHANCED with stream tests
    DataPipeline.test.ts      # NEW tests
    DataWriter.test.ts        # Tests for writer abstraction
  layers/
    DataWriterTest.ts         # Mock writer for testing
  integration/
    DataPipelineLive.integration.test.ts
data/
  greeks/
    2024-03-14/              # Organized by expiration
      SPXW_20240314.csv
      SPXW_20240314.parquet  # Future: Parquet files
```

### Testing Standards

**Test Framework:** Bun test runner with Effect.Stream testing utilities

**Key Testing Focus:**
```typescript
// Test that streaming and batch methods produce equivalent results
it('should produce same results as batch method', async () => {
  const batchResults = await Effect.runPromise(processor.processBulkGreeks(params))
  const streamResults = await Effect.runPromise(
    Stream.runCollect(processor.streamBulkGreeks(params))
  )
  
  expect(streamResults.length).toBe(batchResults.results.length)
  expect(streamResults.map(r => r.recordCount))
    .toEqual(batchResults.results.map(r => r.recordCount))
})

// Test memory efficiency
it('should not accumulate memory', async () => {
  const initialMemory = process.memoryUsage().heapUsed
  
  await Effect.runPromise(
    processor.streamBulkGreeks(largeParams).pipe(
      Stream.tap(() => {
        const currentMemory = process.memoryUsage().heapUsed
        expect(currentMemory - initialMemory).toBeLessThan(100_000_000) // 100MB max
      }),
      Stream.runDrain
    )
  )
})
```

## Definition of Done

### Code Quality
- [ ] All TypeScript strict mode checks pass
- [ ] No ESLint/Biome warnings or errors
- [ ] Code follows patterns from Stories 1.1-1.3
- [ ] All exports properly indexed
- [ ] Common logic properly abstracted between batch and stream

### Testing
- [ ] Unit tests achieve >90% coverage for new code
- [ ] Streaming method tests added to BulkGreeksProcessor
- [ ] DataPipeline service fully tested
- [ ] Memory usage validated to stay bounded
- [ ] CSV output integrity verified
- [ ] Integration tests pass with real data
- [ ] Batch vs stream equivalence verified

### Documentation
- [ ] Service interfaces documented with JSDoc
- [ ] Streaming patterns documented
- [ ] Configuration options explained
- [ ] DataPipeline usage examples provided

### Performance
- [ ] Processes >5,000 records/second
- [ ] Memory usage stays under 200MB regardless of total dataset size
- [ ] Each expiration's data freed after writing
- [ ] CSV writes are atomic and corruption-resistant

## Dependencies & Risks

### Dependencies
- Story 1.1: Project Setup (✅ Complete)
- Story 1.2: ThetaData Client (✅ Complete) 
- Story 1.3: Bulk Greeks Processor (✅ Complete)
- Node.js fs.createWriteStream for file operations
- Effect.Stream for streaming primitives
- Optional: zlib for compression support

### Risks
- **API Changes**: Modifying BulkGreeksProcessor could affect existing code
  - *Mitigation*: Keep backward compatibility, existing method unchanged
- **Memory Leaks**: Stream resources not properly cleaned up
  - *Mitigation*: Use Effect.acquireRelease, test resource cleanup
- **File System Issues**: Disk full, permissions, concurrent writes
  - *Mitigation*: Pre-check disk space, atomic writes, proper error handling
- **Performance**: Stream overhead might reduce throughput
  - *Mitigation*: Benchmark both approaches, optimize chunk sizes

## Notes for Development

1. Start by enhancing BulkGreeksProcessor with streaming method
2. Abstract common logic to avoid duplication
3. Ensure backward compatibility with existing code
4. Focus on memory efficiency over raw speed
5. CSV is foundation for Parquet - same patterns apply
6. Test with real Terminal data early and often

## Validation Checklist

Before marking complete:
- [ ] BulkGreeksProcessor.streamBulkGreeks method works correctly
- [ ] DataPipeline service processes streams to CSV
- [ ] Memory usage stays bounded regardless of dataset size
- [ ] Each expiration's data is freed after processing
- [ ] CSV files are valid and complete
- [ ] Batch and stream methods produce equivalent results
- [ ] All unit tests pass
- [ ] Integration tests pass with Terminal
- [ ] No memory leaks detected
- [ ] Performance meets targets (>5000 records/sec)

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-08-07 | 1.0 | Initial story creation | Sarah (PO) |

## Dev Agent Record

*To be populated during development*

## QA Results

*To be populated during QA review*