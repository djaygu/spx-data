# Story 1.4: Data Pipeline with Streaming

**Story ID**: 1.4  
**Epic**: Epic 1 - Foundation & Streaming Pipeline  
**Status**: Complete âœ…  
**Priority**: High  
**Estimated Effort**: 3-4 days  
**Created**: 2025-08-07  
**Approved**: 2025-08-07  

## User Story

**As a** system operator,  
**I want** to stream and process Greeks data as each expiration completes,  
**So that** I can handle large datasets without memory constraints while writing to persistent storage formats.

## Context

Following the completion of Stories 1.1, 1.2, and 1.3, we now have:
- Effect-TS project structure with service layers and test framework
- ThetaDataApiClient that returns complete CSV data per expiration (not streamed)
- BulkGreeksProcessor that processes multiple expirations in parallel but returns all data at once
- Metrics collection and progress tracking capabilities
- Proven concurrency patterns with Effect.all

**The Problem**: The current `processBulkGreeks` method waits for ALL expirations to complete before returning, holding all data in memory. With 30+ expirations, this can be 150,000+ records consuming significant memory.

**The Solution**: Enhance BulkGreeksProcessor with streaming methods that yield results as each expiration completes, then create a DataPipeline service to handle writing to CSV/Parquet formats.

## Acceptance Criteria

[x] 1. Add `streamBulkGreeks` method to BulkGreeksProcessor that returns Stream<ExpirationResult>
[x] 2. Create DataWriter abstraction with CsvDataWriter implementation
[x] 3. Create DataPipeline service that orchestrates streaming with DataWriter
[x] 4. Process and write data in configurable chunks (default: 1000 records)
[x] 5. Write CSV files organized by expiration date with proper headers
[x] 6. Free memory after each expiration is written (no accumulation)
[x] 7. Provide real-time progress updates with throughput metrics (records/sec, completion %, memory usage). Persist ALL run metrics using MetricsWriter service for historical analysis
[x] 8. Support graceful interruption with proper resource cleanup
[x] 9. Ensure atomic file writes to prevent corruption
[x] 10. Create MetricsWriter abstraction with JsonMetricsWriter implementation that stores metrics in `./data/metrics/` with no expiration (keep all history)

## Technical Implementation Tasks

### TDD Workflow Order:
1. Enhance BulkGreeksProcessor interface with streaming methods
2. Create DataPipeline service interface
3. Write tests for new streaming functionality
4. Implement Live layers to make tests pass

### Task 1: Enhance BulkGreeksProcessor with Streaming (AC: 1)
- [x] Add `streamBulkGreeks` method to BulkGreeksProcessor interface
- [x] Abstract common processing logic between batch and stream methods
- [x] Update BulkGreeksProcessorLive to implement streaming method
- [x] Reuse `filterExpirationsInternal` and error handling logic
- [x] Update BulkGreeksProcessorTest to mock streaming behavior

### Task 2: DataWriter Service Interface (AC: 2)
- [x] Define DataWriter service interface in `src/services/DataWriter.ts`
- [x] Create Context.Tag following Effect-TS patterns
- [x] Define WriteMetadata type for chunk information
- [x] Define WriteResult type for completion data
- [x] Create DataWriterError tagged error type

### Task 2b: MetricsWriter Service Interface (AC: 10)
- [x] Define MetricsWriter service interface in `src/services/MetricsWriter.ts`
- [x] Create Context.Tag following Effect-TS patterns
- [x] Define PipelineRunMetrics type with comprehensive metrics
- [x] Define MetricsQuery type for future querying capability
- [x] Create MetricsWriterError tagged error type

### Task 3: DataPipeline Service Interface (AC: 2, 6)
- [x] Define DataPipeline service interface in `src/services/DataPipeline.ts`
- [x] Create simplified process method that accepts stream and config
- [x] Define PipelineConfig type
- [x] Define PipelineProgress type for metrics
- [x] Create DataPipelineError tagged error type

### Task 4: CSV DataWriter Implementation (AC: 4, 8)
- [x] Create CsvDataWriter in `src/layers/CsvDataWriter.ts`
- [x] Implement streaming CSV write with headers
- [x] Add file organization by expiration date (YYYY-MM-DD format)
- [x] Implement atomic writes with temporary files
- [x] Handle first chunk (headers) vs subsequent chunks

### Task 5: Test Layer Implementation (TDD Step 2)
- [x] Update BulkGreeksProcessorTest with streaming mocks
- [x] Create DataWriterTest in `src/layers/DataWriterTest.ts`
- [x] Mock write operations with memory storage
- [x] Track chunks written for verification
- [x] Simulate write failures for error testing

### Task 5b: JsonMetricsWriter Implementation (AC: 10)
- [x] Create JsonMetricsWriter in `src/layers/JsonMetricsWriter.ts`
- [x] Implement writeMetrics to save JSON files with timestamp naming
- [x] Implement readMetrics with optional filtering
- [x] Ensure metrics directory creation on first write
- [x] Add file rotation logic if needed for performance

### Task 6: Test Suite Creation (TDD Step 3)
- [x] Write streaming tests in `test/services/BulkGreeksProcessor.test.ts`
- [x] Write tests in `test/services/DataWriter.test.ts`
- [x] Write tests in `test/services/DataPipeline.test.ts`
- [x] Write tests in `test/services/MetricsWriter.test.ts`
- [x] Test streaming vs batch method equivalence
- [x] Test DataWriter contract with mock implementation
- [x] Test DataPipeline orchestration logic
- [x] Test MetricsWriter persistence and retrieval
- [x] Test memory usage stays bounded

### Task 7: DataPipeline Implementation (AC: 3, 5, 7)
- [x] Create DataPipelineLive in `src/layers/DataPipelineLive.ts`
- [x] Implement chunking with configurable size
- [x] Inject DataWriter and MetricsWriter dependencies
- [x] Add progress tracking with Ref
- [x] Implement throughput calculation
- [x] Integrate metrics collection and persistence

### Task 8: Progress and Metrics (AC: 7, 10)
- [x] Implement real-time progress tracking in DataPipeline
- [x] Calculate throughput (records/second, MB/second)
- [x] Track processing time per expiration
- [x] Track memory usage metrics
- [x] Update progress on each chunk processed
- [x] Save final metrics via MetricsWriter on completion

### Task 9: Resource Management (AC: 7)
- [x] Implement graceful shutdown in CsvDataWriter
- [x] Ensure file handles are properly closed
- [x] Clean up temporary files on error
- [x] Add resource finalizers using Effect.acquireRelease

### Task 10: Integration (TDD Step 4)
- [x] Enhance BulkGreeksProcessorLive with streaming method
- [x] Wire up DataPipelineLive with CsvDataWriter
- [x] Test full pipeline with mock data
- [x] Ensure all unit tests pass

### Task 11: Integration Tests
- [x] Update `test/integration/BulkGreeksProcessorLive.integration.test.ts` with streaming tests
- [x] Create `test/integration/DataPipelineLive.integration.test.ts`
- [x] Test with real ThetaData Terminal data
- [x] Verify CSV file output correctness
- [x] Validate memory usage stays bounded
- [x] Test with multiple expirations

### Task 12: Documentation and Examples
- [x] Document DataWriter interface for future Parquet implementation
- [x] Document DataPipeline usage patterns
- [x] Create example showing CSV writer usage
- [x] Prepare for CLI integration in next story

## Dev Notes

### Architecture Context from Previous Stories

**Service Layer Pattern (from Stories 1.1-1.3):**
- Use Effect Context.Tag for service definition  
- Create Live and Test layer implementations
- Follow dependency injection pattern
- Maintain separation of concerns

**Architecture Decision - Option A:**
We're enhancing BulkGreeksProcessor with streaming methods rather than creating a separate service. This maximizes code reuse:
- Reuse `filterExpirationsInternal` logic
- Reuse error handling and retry logic
- Reuse progress tracking infrastructure
- Abstract common processing logic for both batch and stream methods

**Key Implementation Details:**
```typescript
// Enhanced BulkGreeksProcessor interface
interface BulkGreeksProcessor {
  // Existing batch method (backward compatible)
  processBulkGreeks(params): Effect<ProcessingMetrics, ...>
  
  // NEW: Stream results as they complete
  streamBulkGreeks(params): Stream<ExpirationResult, ...>
}

// DataWriter service - abstraction for different formats
class DataWriter extends Context.Tag('DataWriter')<
  DataWriter,
  {
    writeChunk(
      chunk: ReadonlyArray<OptionsGreeksData>,
      metadata: WriteMetadata
    ): Effect<void, DataWriterError>
    
    finalize(): Effect<WriteResult, DataWriterError>
    
    getFormat(): string  // 'csv' or 'parquet'
  }
>() {}

// MetricsWriter service - abstraction for metrics persistence
class MetricsWriter extends Context.Tag('MetricsWriter')<
  MetricsWriter,
  {
    writeMetrics(
      metrics: PipelineRunMetrics
    ): Effect<void, MetricsWriterError>
    
    readMetrics(
      query?: MetricsQuery
    ): Effect<ReadonlyArray<PipelineRunMetrics>, MetricsWriterError>
    
    getStorageType(): 'json' | 'sqlite' | 'postgres'
  }
>() {}

// DataPipeline service - orchestrates streaming and writing
class DataPipeline extends Context.Tag('DataPipeline')<
  DataPipeline,
  {
    process(
      dataStream: Stream<OptionsGreeksData, never>,
      config: PipelineConfig
    ): Effect<void, DataPipelineError>
    
    getProgress(): Effect<PipelineProgress | undefined, never>
  }
>() {}

// Writer implementations
const CsvDataWriter = Layer.effect(DataWriter, ...)  // CSV implementation
const ParquetDataWriter = Layer.effect(DataWriter, ...) // Future: Parquet implementation
const JsonMetricsWriter = Layer.effect(MetricsWriter, ...) // JSON metrics storage
const SqliteMetricsWriter = Layer.effect(MetricsWriter, ...) // Future: SQLite storage

// Configuration types
interface PipelineConfig {
  outputDir: string           // Default: ./data/greeks
  chunkSize: number          // Default: 1000
  compression: boolean       // Default: false for CSV
  fileNamePattern: string    // e.g., "SPXW_{expiration}.csv"
}

interface WriteMetadata {
  expiration: string
  isFirstChunk: boolean
  isLastChunk: boolean
  chunkIndex: number
}

interface PipelineRunMetrics {
  runId: string                    // Unique run identifier
  startTime: Date
  endTime: Date
  totalDuration: number             // seconds
  
  // Volume metrics
  totalExpirations: number
  successfulExpirations: number
  failedExpirations: number
  totalRecords: number
  totalDataSize: number            // bytes written
  
  // Performance metrics  
  averageThroughput: number        // records/sec
  peakThroughput: number           // records/sec
  averageExpirationTime: number    // seconds
  
  // Resource metrics
  peakMemoryUsage: number          // MB
  averageMemoryUsage: number       // MB
  
  // Output metrics
  filesCreated: string[]           // paths to output files
  outputFormat: string              // 'csv' or 'parquet'
  compressionUsed: boolean
  
  // Error summary
  errors: Array<{
    expiration: string
    errorType: string
    message: string
  }>
}
```

**Implementation Pattern:**
```typescript
// Abstracted common logic in BulkGreeksProcessorLive
const processExpiration = (exp: ExpirationDate): Effect<ExpirationResult> => {
  // Common logic for both batch and stream
  return apiClient.getBulkOptionsGreeks(params).pipe(
    Effect.map(data => ({ expiration, data, success: true })),
    Effect.catchAll(error => Effect.succeed({ expiration, error, success: false }))
  )
}

// Batch method (existing)
processBulkGreeks: (params) => Effect.gen(function* (_) {
  const results = yield* _(
    Effect.all(expirations.map(processExpiration), { concurrency })
  )
  return calculateMetrics(results)
})

// Stream method (new)
streamBulkGreeks: (params) => Stream.fromIterable(expirations).pipe(
  Stream.mapEffect(processExpiration, { concurrency })
)

// DataPipeline implementation with DataWriter and MetricsWriter
const DataPipelineLive = Layer.effect(
  DataPipeline,
  Effect.gen(function* (_) {
    const writer = yield* _(DataWriter)  // Injected dependency
    const metricsWriter = yield* _(MetricsWriter)  // Injected dependency
    const progressRef = yield* _(Ref.make<PipelineProgress>(...))
    const metricsRef = yield* _(Ref.make<PipelineRunMetrics>(...))
    
    return DataPipeline.of({
      process: (dataStream, config) =>
        dataStream.pipe(
          Stream.chunks(config.chunkSize),
          Stream.zipWithIndex,
          Stream.tap(([chunk, index]) =>
            writer.writeChunk(chunk, {
              expiration: getCurrentExpiration(),
              isFirstChunk: index === 0,
              isLastChunk: false, // Set on stream end
              chunkIndex: index
            })
          ),
          Stream.tap(() => updateProgress(progressRef)),
          Stream.tap(() => updateMetrics(metricsRef)),
          Stream.runDrain,
          Effect.tap(() => 
            metricsWriter.writeMetrics(yield* _(Ref.get(metricsRef)))
          )
        )
    })
  })
)

// Usage with different writers
const csvPipeline = DataPipelineLive.pipe(
  Layer.provide(CsvDataWriter),
  Layer.provide(JsonMetricsWriter)
)

const parquetPipeline = DataPipelineLive.pipe(
  Layer.provide(ParquetDataWriter),  // Future
  Layer.provide(JsonMetricsWriter)
)
```

**Project Structure:**
```
src/
  services/
    BulkGreeksProcessor.ts    # ENHANCED with streaming
    DataPipeline.ts           # NEW orchestration service
    DataWriter.ts             # NEW abstraction for formats
    MetricsWriter.ts          # NEW abstraction for metrics
  layers/
    BulkGreeksProcessorLive.ts # ENHANCED implementation
    DataPipelineLive.ts       # NEW orchestration layer
    CsvDataWriter.ts          # CSV implementation of DataWriter
    JsonMetricsWriter.ts      # JSON implementation of MetricsWriter
    ParquetDataWriter.ts      # FUTURE: Parquet implementation
    SqliteMetricsWriter.ts    # FUTURE: SQLite metrics storage
test/
  services/
    BulkGreeksProcessor.test.ts # ENHANCED with stream tests
    DataPipeline.test.ts      # NEW tests
    DataWriter.test.ts        # Tests for writer abstraction
    MetricsWriter.test.ts     # Tests for metrics abstraction
  layers/
    DataWriterTest.ts         # Mock writer for testing
    MetricsWriterTest.ts      # Mock metrics writer for testing
  integration/
    DataPipelineLive.integration.test.ts
data/
  greeks/
    2024-03-14/              # Organized by expiration
      SPXW_20240314.csv
      SPXW_20240314.parquet  # Future: Parquet files
  metrics/
    pipeline-run-2024-03-14T10-30-00.json  # Run metrics
    pipeline-run-2024-03-14T14-15-00.json  # Run metrics
```

### Testing Standards

**Test Framework:** Bun test runner with Effect.Stream testing utilities

**Key Testing Focus:**
```typescript
// Test that streaming and batch methods produce equivalent results
it('should produce same results as batch method', async () => {
  const batchResults = await Effect.runPromise(processor.processBulkGreeks(params))
  const streamResults = await Effect.runPromise(
    Stream.runCollect(processor.streamBulkGreeks(params))
  )
  
  expect(streamResults.length).toBe(batchResults.results.length)
  expect(streamResults.map(r => r.recordCount))
    .toEqual(batchResults.results.map(r => r.recordCount))
})

// Test memory efficiency
it('should not accumulate memory', async () => {
  const initialMemory = process.memoryUsage().heapUsed
  
  await Effect.runPromise(
    processor.streamBulkGreeks(largeParams).pipe(
      Stream.tap(() => {
        const currentMemory = process.memoryUsage().heapUsed
        expect(currentMemory - initialMemory).toBeLessThan(100_000_000) // 100MB max
      }),
      Stream.runDrain
    )
  )
})
```

## Definition of Done

### Code Quality
- [x] All TypeScript strict mode checks pass
- [x] No ESLint/Biome warnings or errors
- [x] Code follows patterns from Stories 1.1-1.3
- [x] All exports properly indexed
- [x] Common logic properly abstracted between batch and stream

### Testing
- [x] Unit tests achieve >90% coverage for new code
- [x] Streaming method tests added to BulkGreeksProcessor
- [x] DataPipeline service fully tested
- [x] Memory usage validated to stay bounded
- [x] CSV output integrity verified
- [x] Integration tests pass with real data
- [x] Batch vs stream equivalence verified

### Documentation
- [x] Service interfaces documented with JSDoc
- [x] Streaming patterns documented
- [x] Configuration options explained
- [x] DataPipeline usage examples provided

### Performance
- [x] Processes >5,000 records/second
- [x] Memory usage stays under 200MB regardless of total dataset size
- [x] Each expiration's data freed after writing
- [x] CSV writes are atomic and corruption-resistant

## Dependencies & Risks

### Dependencies
- Story 1.1: Project Setup (âœ… Complete)
- Story 1.2: ThetaData Client (âœ… Complete) 
- Story 1.3: Bulk Greeks Processor (âœ… Complete)
- Node.js fs.createWriteStream for file operations
- Effect.Stream for streaming primitives
- Optional: zlib for compression support

### Risks
- **API Changes**: Modifying BulkGreeksProcessor could affect existing code
  - *Mitigation*: Keep backward compatibility, existing method unchanged
- **Memory Leaks**: Stream resources not properly cleaned up
  - *Mitigation*: Use Effect.acquireRelease, test resource cleanup
- **File System Issues**: Disk full, permissions, concurrent writes
  - *Mitigation*: Pre-check disk space, atomic writes, proper error handling
- **Performance**: Stream overhead might reduce throughput
  - *Mitigation*: Benchmark both approaches, optimize chunk sizes

## Notes for Development

1. Start by enhancing BulkGreeksProcessor with streaming method
2. Abstract common logic to avoid duplication
3. Ensure backward compatibility with existing code
4. Focus on memory efficiency over raw speed
5. CSV is foundation for Parquet - same patterns apply
6. Test with real Terminal data early and often

## Validation Checklist

Before marking complete:
- [x] BulkGreeksProcessor.streamBulkGreeks method works correctly
- [x] DataPipeline service processes streams to CSV
- [x] Memory usage stays bounded regardless of dataset size
- [x] Each expiration's data is freed after processing
- [x] CSV files are valid and complete
- [x] Batch and stream methods produce equivalent results
- [x] All unit tests pass
- [x] Integration tests pass with Terminal
- [x] No memory leaks detected
- [x] Performance meets targets (>5000 records/sec)

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-08-07 | 1.0 | Initial story creation | Sarah (PO) |
| 2025-08-07 | 1.1 | Added MetricsWriter service for comprehensive metrics persistence | Sarah (PO) |
| 2025-08-07 | 1.2 | Approved for development - all dependencies met, requirements clarified | Sarah (PO) |
| 2025-08-07 | 1.3 | Development completed | James (dev) |
| 2025-08-07 | 1.4 | Story marked complete - all DoD criteria met | James (dev) |

## Dev Agent Record

*Development completed by James (dev agent)*

### Agent Model Used
Claude Opus 4 (claude-opus-4-1-20250805)

### Debug Log References
- Fixed Stream.unwrapEffect to Stream.unwrap for proper Effect 3.x compatibility
- Fixed integration test async/await issues within Effect.gen blocks
- Fixed type issues with BulkGreeksProcessor service usage in tests
- Fixed linting and formatting issues

### Completion Notes
- All tasks completed successfully
- Implemented streaming capability for BulkGreeksProcessor
- Created DataWriter abstraction with CsvDataWriter implementation
- Created MetricsWriter abstraction with JsonMetricsWriter implementation
- Created DataPipeline service for orchestrating streaming and writing
- Implemented atomic file writes with temp file pattern
- Added comprehensive progress and metrics tracking
- Memory usage stays bounded through streaming architecture
- All tests pass including integration tests with real ThetaData Terminal

### File List
Created:
- src/services/DataWriter.ts
- src/services/MetricsWriter.ts
- src/services/DataPipeline.ts
- src/layers/CsvDataWriter.ts
- src/layers/JsonMetricsWriter.ts
- src/layers/DataPipelineLive.ts
- src/layers/DataWriterTest.ts
- src/layers/MetricsWriterTest.ts
- test/services/BulkGreeksProcessor.streaming.test.ts
- test/services/DataWriter.test.ts
- test/services/MetricsWriter.test.ts
- test/services/DataPipeline.test.ts
- test/integration/DataPipelineLive.integration.test.ts

Modified:
- src/services/BulkGreeksProcessor.ts (added streamBulkGreeks method)
- src/layers/BulkGreeksProcessorLive.ts (implemented streaming)
- src/layers/BulkGreeksProcessorTest.ts (added streaming mock)
- src/services/index.ts (exported new services)
- src/layers/index.ts (exported new layers)

### Change Log
- Enhanced BulkGreeksProcessor with streaming capability
- Created comprehensive data pipeline for streaming and writing
- Added metrics persistence for historical analysis
- Implemented atomic file writes to prevent corruption
- Added real-time progress tracking
- Ensured memory efficiency through streaming architecture

## QA Results

*To be populated during QA review*