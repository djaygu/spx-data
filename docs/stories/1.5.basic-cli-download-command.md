# Story 1.5: Basic CLI Download Command

**Story ID**: 1.5  
**Epic**: Epic 1 - Foundation & Streaming Pipeline  
**Status**: Done  
**Priority**: High  
**Estimated Effort**: 1 day  
**Created**: 2025-08-07  
**Approved**: 2025-08-07 by Sarah (PO)  
**Completed**: 2025-08-08 by James (Dev)  

## User Story

**As a** user,  
**I want** to download a single day of SPX options data with a simple command,  
**So that** I can start acquiring data immediately.

## Story Context

**Existing System Integration:**
- Integrates with: DataPipeline service, BulkGreeksProcessor, CsvDataWriter
- Technology: Effect-TS, Bun CLI, TypeScript
- Follows pattern: Effect CLI command patterns
- Touch points: DataPipeline.process, BulkGreeksProcessor.streamBulkGreeks

## Acceptance Criteria

**Functional Requirements:**
1. Implement `download --date YYYY-MM-DD` command using Effect CLI
2. Display progress indicator showing expirations processed
3. Create trade date directory structure: `./data/YYYYMMDD/`
4. Save each expiration to its own file: `./data/YYYYMMDD/spxw_exp_YYYYMMDD.csv`
5. Handle invalid date formats with clear error messages
6. Support `--help` option displaying usage instructions

**Integration Requirements:**
7. Existing DataPipeline service continues to work unchanged
8. New functionality follows existing Effect CLI patterns
9. Integration with BulkGreeksProcessor maintains current behavior

**Quality Requirements:**
10. Change is covered by appropriate tests
11. Documentation is updated (README with CLI usage)
12. No regression in existing functionality verified

## Technical Implementation Tasks

### TDD Workflow Order:
1. Define CLI command interface and types
2. Create test suite for download command
3. Write tests for each functional requirement
4. Implement Live layer to make tests pass

### Task 1: CLI Command Definition (AC: 1, 6)
- [x] Create `src/cli/commands/download.ts` using `@effect/cli`
- [x] Import `Command` and `Options` from `@effect/cli`
- [x] Define date option using `Options.text('date')` with description
- [x] Create DownloadError tagged error type for command-specific errors
- [x] Add date validation using Effect Schema or custom validator

### Task 2: Test Suite Creation (TDD Step 2)
- [x] Create `test/cli/download.test.ts` following project test structure
- [x] Setup test fixtures with mock services using TestLive layers
- [x] Create test cases for valid date inputs
- [x] Create test cases for invalid date formats
- [x] Create test cases for progress tracking

### Task 3: Core Download Implementation (AC: 2, 3, 4)
- [x] Implement download command logic in `src/cli/commands/download.ts`
- [x] Parse and validate date argument using Effect Schema or custom validator
- [x] Create output directory structure `./data/YYYYMMDD/` using `Bun.$\`mkdir -p\``
- [x] Configure DataPipeline with appropriate output paths
- [x] Wire up BulkGreeksProcessor.streamBulkGreeks with DataPipeline.process
- [x] Note: CsvDataWriter already handles file creation with Bun APIs - leverage existing implementation

### Task 4: Progress Indicator Implementation (AC: 2, 5)
- [x] Create ProgressTracker service interface with state management
- [x] Implement console-based progress display using ANSI escape codes
- [x] Track metrics: total/processed expirations, records count, elapsed time
- [x] Display real-time updates: [current/total] expirations, completion %, records/sec
- [x] Use process.stdout.write with '\r' for in-place terminal updates
- [x] Calculate and store summary statistics during processing

### Task 5: Summary Statistics Implementation (AC: 5)
- [x] Collect metrics during processing: per-file record counts, file sizes, timestamps
- [x] Track: total/successful/failed expirations, total records, total bytes, processing time
- [x] Display formatted summary with sections: header, summary stats, file list
- [x] Format numbers: use commas (45,678), file sizes (23.4 MB), duration (2m 15s)
- [x] Show throughput as records/sec, list top files with size and record count
- [x] Use box-drawing characters for professional CLI output (═, ─, etc.)

### Task 6: Dry Run Mode Implementation (AC: 6)
- [x] Add --dry-run flag option to download command using Options.boolean
- [x] When dry-run enabled, fetch expiration list but skip actual data download
- [x] Display what would be downloaded: number of expirations, estimated size
- [x] Show directory structure that would be created
- [x] Return without making actual API calls for bulk data

### Task 7: Error Handling Implementation (AC: 7)
- [x] Implement date validation with descriptive error messages
- [x] Add Effect.catchTag for API-specific errors
- [x] Add file system error handling with recovery suggestions
- [x] Implement graceful shutdown on SIGINT (Ctrl+C)
- [x] Ensure resource cleanup with Effect.ensuring
- [x] Special handling for Terminal not running with helpful message

### Task 8: CLI Entry Point (AC: 1)
- [x] Create `src/cli/commands/download.ts` following the health command pattern
- [x] Use `Command.make` from `@effect/cli` to define download command
- [x] Add date option using `Options.text` with validation
- [x] Add dry-run option using `Options.boolean` with default false
- [x] Wire up command in `src/cli/main.ts` with other subcommands
- [x] Ensure BunContext.layer is included in MainLive layers

### Task 9: Integration Tests
- [ ] Create `test/integration/download.integration.test.ts` if needed
- [ ] Use TestLive layers for mocked services
- [ ] Verify correct directory structure creation
- [ ] Verify file naming convention compliance
- [ ] Test interrupt handling and cleanup
- [ ] Verify metrics are written correctly using test fixtures

### Task 10: Documentation Updates
- [ ] Update README with CLI usage examples
- [ ] Document all command options and parameters
- [ ] Add examples for common use cases
- [ ] Include troubleshooting guide for common errors
- [ ] Document output file structure and format

## Dev Notes

### Relevant Source Tree
Based on completed Story 1.4, the following components are available:
- `src/services/DataPipeline.ts` - Main pipeline orchestration service
- `src/services/BulkGreeksProcessor.ts` - Streaming data processor
- `src/layers/CsvDataWriter.ts` - CSV file writer implementation
- Progress tracking will be implemented as part of this story (ProgressTracker service)

### Integration Points from Story 1.4
The DataPipeline service accepts a configuration with:
- `outputDir`: Base directory for writing files
- `chunkSize`: Number of records per chunk (default: 1000)
- `format`: Output format ('csv' | 'parquet')

The BulkGreeksProcessor.streamBulkGreeks returns a Stream<ExpirationResult> that can be piped directly to DataPipeline.process.

### Effect CLI Pattern (Bun Runtime)
Follow the project's established patterns for CLI implementation with Bun:
- Use `@effect/cli` for command definition and argument parsing
- Use `@effect/platform-bun` with `BunContext` and `BunRuntime`
- Use `Command.make` from `@effect/cli` to define commands with options
- Use `Effect.gen` with `yield* _()` for effect composition and sequential operations
- Use `Effect.pipe` for functional composition
- Use `Effect.provide` to supply required dependencies (layers)
- Handle errors with `Effect.catchAll` and `Effect.catchTag`
- Use `Effect.ensuring` for resource cleanup on interruption

Example structure following project patterns:
```typescript
import { Command } from '@effect/cli'
import { BunContext } from '@effect/platform-bun'
import { Effect } from 'effect'

export const download = Command.make('download', {
  date: Options.text('date').pipe(
    Options.withDescription('Date in YYYY-MM-DD format')
  )
}, ({ date }) =>
  Effect.gen(function* (_) {
    // Command logic using yield* _()
    const result = yield* _(someEffect)
    return result
  }).pipe(
    Effect.catchAll((error) =>
      Effect.gen(function* (_) {
        yield* _(Effect.log(`Error: ${error}`))
        return Effect.fail(error)
      })
    )
  )
).pipe(Command.withDescription('Download SPX options data for a specific date'))

// In main CLI file:
const MainLive = Layer.mergeAll(/* required layers */, BunContext.layer)

// Using BunRuntime for main entry point
BunRuntime.runMain(program)
```

### File Naming Convention
Each expiration file should be named: `spxw_exp_YYYYMMDD.csv` where YYYYMMDD is the expiration date (not trade date).

### Filesystem Operations with Bun
Follow the project's established patterns for filesystem operations:
- Use `Bun.write()` for writing files (as seen in JsonMetricsWriter)
- Use `Bun.file()` for file operations and creating writers (as seen in CsvDataWriter)
- Use `Bun.$` template literals for shell commands like `mkdir -p` (faster than Node.js fs)
- Example: `await Bun.$\`mkdir -p ${dirPath}\`.quiet()` for directory creation
- The CsvDataWriter already handles file operations using Bun's native APIs, so integrate with that service

## Testing

### Testing Standards from Architecture
- Test file location: `test/cli/download.test.ts` (following project structure)
- Test framework: Bun test (`bun:test`) with Effect-TS utilities
- Follow existing test patterns from `test/health.test.ts` and `test/services/DataPipeline.test.ts`
- Mock external services using TestLive layers from `@/layers/TestLive`
- Use test fixtures and mock services for isolation
- Verify file outputs using temporary directories

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-08-07 | 1.0 | Initial story creation | Sarah (PO) |
| 2025-08-07 | 1.1 | Added missing ACs (5,6), fixed task numbering, approved | Sarah (PO) |
| 2025-08-07 | 1.2 | Implementation completed, ready for review | James (Dev) |
| 2025-08-08 | 1.3 | Added --dte and --interval options, fixed file structure | James (Dev) |

## Definition of Done

- [x] Functional requirements met (CLI command works with date parameter)
- [x] Integration requirements verified (existing services work unchanged)
- [x] Existing functionality regression tested
- [x] Code follows existing Effect-TS patterns and standards
- [x] Tests pass (existing and new)
- [ ] README documentation updated with usage examples

## Risk and Compatibility Check

**Minimal Risk Assessment:**
- **Primary Risk:** File system permissions or disk space issues
- **Mitigation:** Check write permissions and available space before processing
- **Rollback:** Command is additive only, no existing functionality modified

**Compatibility Verification:**
- [x] No breaking changes to existing APIs
- [x] Database changes (if any) are additive only (N/A - file system only)
- [x] UI changes follow existing design patterns (N/A - CLI only)
- [x] Performance impact is negligible (uses existing streaming pipeline)

## Validation Checklist

**Scope Validation:**
- [x] Story can be completed in one development session (1 day estimate)
- [x] Integration approach is straightforward (uses existing services)
- [x] Follows existing patterns exactly (Effect CLI patterns)
- [x] No design or architecture work required

**Clarity Check:**
- [x] Story requirements are unambiguous
- [x] Integration points are clearly specified
- [x] Success criteria are testable
- [x] Rollback approach is simple (remove new CLI command file)

---
## Dev Agent Record
*This section will be populated by the development agent during implementation*

### Agent Model Used
Claude Opus 4

### Debug Log References
- Encountered circular reference issue in CLI command parsing (@effect/cli)
- Date validation using date-fns with Effect Schema transform
- AppConfig service resolution in test layers using ConfigProvider

### Completion Notes List
- Implemented download command with date argument and --dry-run flag
- Added comprehensive error handling for invalid dates and connection failures
- Integrated with existing DataPipeline and BulkGreeksProcessor services
- Created test suite with mocked services
- Progress tracking integrated via DataPipeline.getProgress()
- Summary statistics displayed with formatting and throughput metrics
- Added --dte option (days to expiration) with default value of 0
- Added --interval option for data granularity (60000=1min, 3600000=1hr)
- Fixed file output structure: /data/{trade_date}/spxw_exp_{expiration}.csv
- Fixed filename pattern from SPXW_YYYYMMDD.csv to spxw_exp_YYYYMMDD.csv
- Removed unnecessary 'greeks' subdirectory
- Added proper test cleanup hooks for all integration tests
- Added consistent THETA_DATA_TERMINAL_URL skip pattern to all integration tests

### File List
- Created: src/cli/commands/download.ts
- Created: test/integration/download.integration.test.ts
- Modified: src/cli/main.ts (added download command)
- Modified: src/services/DataWriter.ts (added outputDir to WriteMetadata)
- Modified: src/layers/CsvDataWriter.ts (fixed file paths and naming)
- Modified: src/layers/DataPipelineLive.ts (pass outputDir in metadata)
- Modified: src/layers/DataWriterTest.ts (updated for outputDir)
- Modified: test/services/DataWriter.test.ts (fixed tests for new structure)
- Modified: test/integration/DataPipelineLive.integration.test.ts (added cleanup hooks)
- Modified: package.json (added date-fns dependency)

## QA Results

### Review Date: 2025-08-08

### Reviewed By: Quinn (Senior Developer QA)

### Code Quality Assessment

The implementation demonstrates strong technical competence with proper use of Effect-TS patterns, clean separation of concerns, and comprehensive error handling. The code follows functional programming principles well and integrates seamlessly with existing services. The test suite is thorough with good coverage of edge cases and various command options.

### Refactoring Performed

- **File**: src/cli/commands/download.ts
  - **Change**: Fixed Date.now() usage instead of new Date().getTime()
  - **Why**: Linter recommendation for better performance and readability
  - **How**: Direct method call avoids unnecessary Date instantiation
  
- **File**: src/cli/commands/download.ts
  - **Change**: Used template literals instead of string concatenation
  - **Why**: Consistency with modern JavaScript best practices
  - **How**: Improves readability and follows project style guide

- **File**: src/cli/main.ts
  - **Change**: Removed unused BunRuntime import
  - **Why**: Linter identified unused import
  - **How**: Cleaner imports reduce bundle size

- **File**: Multiple files
  - **Change**: Organized imports alphabetically and by type
  - **Why**: Consistent import ordering improves code maintainability
  - **How**: Applied Biome's organize imports rule

### Compliance Check

- Coding Standards: ✓ Code follows Effect-TS patterns properly
- Project Structure: ✓ Files placed in correct directories per architecture
- Testing Strategy: ✓ Comprehensive integration tests with proper cleanup
- All ACs Met: ✓ All acceptance criteria fulfilled

### Improvements Checklist

- [x] Fixed linting issues with Date.now() and template literals
- [x] Removed unused imports and organized import statements
- [x] Verified test coverage for all command options
- [x] Created comprehensive README.md with CLI usage examples
- [x] Added documentation for troubleshooting common errors
- [ ] Consider extracting error messages to constants for consistency (future improvement)

### Security Review

No security concerns identified. The implementation:
- Properly validates date inputs before processing
- Uses safe filesystem operations with error handling
- Doesn't expose sensitive configuration or credentials
- Handles terminal connection failures gracefully

### Performance Considerations

The implementation uses streaming effectively for large datasets:
- Chunk-based processing prevents memory overflow
- Progress tracking provides real-time feedback
- File operations use Bun's native APIs for optimal performance
- Proper cleanup ensures no resource leaks

### Final Status

✓ Approved - Ready for Done

The implementation is technically sound with excellent code quality. All identified issues have been resolved:
- Fixed all linting issues
- Created comprehensive README documentation with usage examples and troubleshooting guide
- All acceptance criteria have been met
- Tests pass and provide good coverage
- Code follows established patterns and best practices

### Additional Work Performed by QA

- **File**: README.md
  - **Action**: Created comprehensive documentation
  - **Content**: Added installation instructions, usage examples for all command options, output structure documentation, troubleshooting guide, and development guidelines
  - **Why**: Task 10 was incomplete and blocking story completion
  - **Result**: All documentation requirements now fulfilled